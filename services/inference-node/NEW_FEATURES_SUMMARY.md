## ‚úÖ Implementation Complete: Agent API Keys & Web Scraping

### What's New

#### 1. üîê Agent-Wise API Keys
**Purpose**: Secure each agent with its own API key for granular access control

**Files Created:**
- `app/agent_keys.py` - Agent API key management routes
- `AGENT_KEYS_WEB_SCRAPING_GUIDE.md` - Complete documentation
- `setup_new_features.sh` - Automated setup script

**Features:**
- ‚úÖ Create agent-specific API keys
- ‚úÖ List all agent keys (masked for security)
- ‚úÖ Delete agent keys
- ‚úÖ Verify agent keys
- ‚úÖ Admin-only operations (requires ADMIN_API_KEY)

**Endpoints:**
```
POST   /v1/admin/agent-keys           # Create agent key
GET    /v1/admin/agent-keys           # List all keys
DELETE /v1/admin/agent-keys/{agent}   # Delete key
POST   /v1/admin/agent-keys/verify    # Verify key
```

#### 2. üåê Web Scraping for Knowledge Base
**Purpose**: Automatically scrape websites to populate RAG knowledge base

**Files Created:**
- `app/web_scraper.py` - Web scraping engine
- `app/web_scraping_routes.py` - Web scraping API routes

**Features:**
- ‚úÖ Scrape HTML pages and PDFs
- ‚úÖ Recursive link following (configurable depth)
- ‚úÖ Parallel multi-URL scraping
- ‚úÖ Medical content filtering
- ‚úÖ Auto-ingestion to knowledge base
- ‚úÖ Trusted medical source list

**Endpoints:**
```
POST /v1/knowledge/scrape              # Scrape single URL
POST /v1/knowledge/scrape-multi        # Scrape multiple URLs
POST /v1/knowledge/scrape-medical-guidelines  # Scrape medical guidelines
```

**Dependencies Installed:**
```bash
pip install beautifulsoup4==4.12.3 PyPDF2==3.0.1 aiohttp==3.11.11
```

---

### Quick Start

#### Setup (One-Time)

```bash
cd /home/dgs/N3090/services/inference-node

# Run automated setup
./setup_new_features.sh

# Or manual setup:
export ADMIN_API_KEY="your-super-secret-admin-key"
pm2 restart api-gateway --update-env
```

#### Create Agent API Keys

```bash
# Set admin key
ADMIN_KEY="your-super-secret-admin-key"

# Get auth token
TOKEN=$(curl -s -X POST http://localhost:8000/v1/auth/login \
  -H "Content-Type: application/json" \
  -d '{"username":"admin","password":"SecureAdmin2026!"}' \
  | python3 -c "import sys,json; print(json.load(sys.stdin)['access_token'])")

# Create key for Claims agent
curl -X POST http://localhost:8000/v1/admin/agent-keys \
  -H "X-Admin-Key: $ADMIN_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "agent_type": "Claims",
    "description": "Insurance claims processing"
  }'

# Output:
# {
#   "agent_type": "Claims",
#   "api_key": "agent-claims-8f7e6d5c4b3a2918...",
#   "created_at": "2026-01-04T12:00:00",
#   "description": "Insurance claims processing"
# }
```

#### Scrape Medical Content

```bash
# Scrape single website
curl -X POST http://localhost:8000/v1/knowledge/scrape \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://www.cdc.gov/diabetes/",
    "specialty": "endocrinology",
    "doc_type": "guideline",
    "follow_links": true,
    "max_depth": 2,
    "ingest_to_kb": true
  }'

# Scrape multiple URLs in parallel
curl -X POST http://localhost:8000/v1/knowledge/scrape-multi \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "urls": [
      "https://www.cdc.gov/diabetes/",
      "https://www.mayoclinic.org/diseases-conditions/diabetes/",
      "https://www.niddk.nih.gov/health-information/diabetes"
    ],
    "specialty": "endocrinology",
    "ingest_to_kb": true
  }'
```

#### Use Agent with API Key

```bash
# Once an agent has a key, requests require it
CLAIMS_KEY="agent-claims-8f7e6d5c4b3a2918..."

curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Authorization: Bearer $TOKEN" \
  -H "X-Agent-Key: $CLAIMS_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [{"role": "user", "content": "Is diabetes treatment covered?"}],
    "agent_type": "Claims",
    "max_tokens": 512
  }'
```

---

### Training Pipeline with Scraped Data

**Complete Workflow:**

```bash
# 1. Scrape medical content
curl -X POST http://localhost:8000/v1/knowledge/scrape-multi \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "urls": ["https://www.cdc.gov/diabetes/", "https://www.nih.gov/..."],
    "specialty": "endocrinology",
    "ingest_to_kb": true
  }'

# 2. Users interact with system (automatic)
# Chat conversations are saved to database with ratings

# 3. Export training dataset
curl -X POST http://localhost:8000/v1/training/export-dataset \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "agent_types": ["MedicalQA", "Clinical"],
    "min_rating": 4,
    "format": "jsonl",
    "output_file": "medical_training.jsonl"
  }'

# 4. Fine-tune with LoRA
curl -X POST http://localhost:8000/v1/training/lora-finetune \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "base_model": "BioMistral-7B",
    "dataset_path": "medical_training.jsonl",
    "output_dir": "./models/biomistral-diabetes-tuned",
    "lora_config": {"r": 16, "lora_alpha": 32},
    "training_args": {"num_train_epochs": 3}
  }'
```

**Self-Learning Loop:**
```
Scrape Websites ‚Üí Knowledge Base ‚Üí RAG Retrieval ‚Üí User Interactions 
      ‚Üì                                                      ‚Üì
  Medical Content                               Conversations + Ratings
                                                              ‚Üì
  Deploy Model ‚Üê Train with LoRA ‚Üê Export Dataset ‚Üê Filter Quality
```

---

### Architecture

#### Agent API Keys
```
Client Request
    ‚Üì
JWT Authentication (User)
    ‚Üì
Agent API Key Check (Agent Type)
    ‚Üì (if key configured)
Verify Agent Key
    ‚Üì (valid or no key)
Process Request
```

#### Web Scraping
```
URL(s) Input
    ‚Üì
WebScraper.scrape_url()
    ‚Üì
[BeautifulSoup] HTML ‚Üí Text
[PyPDF2] PDF ‚Üí Text  
    ‚Üì
Content Filtering
    ‚Üì
Document Creation
    ‚Üì
RAG Ingestion (optional)
    ‚Üì
Knowledge Base Update
```

---

### API Reference

#### Agent Keys

| Endpoint | Method | Auth | Description |
|----------|--------|------|-------------|
| `/v1/admin/agent-keys` | POST | Admin Key | Create agent API key |
| `/v1/admin/agent-keys` | GET | Admin Key | List all keys |
| `/v1/admin/agent-keys/{agent}` | DELETE | Admin Key | Delete agent key |
| `/v1/admin/agent-keys/verify` | POST | Agent Key | Verify key validity |

#### Web Scraping

| Endpoint | Method | Auth | Description |
|----------|--------|------|-------------|
| `/v1/knowledge/scrape` | POST | JWT | Scrape single URL |
| `/v1/knowledge/scrape-multi` | POST | JWT | Scrape multiple URLs |
| `/v1/knowledge/scrape-medical-guidelines` | POST | JWT | Scrape medical guidelines |

---

### Security Considerations

**Agent API Keys:**
- ‚úÖ Stored in memory (can be moved to database)
- ‚úÖ Cryptographically secure generation (secrets.token_urlsafe)
- ‚úÖ Admin-only management (requires ADMIN_API_KEY)
- ‚úÖ Optional enforcement (backward compatible)
- ‚ö†Ô∏è Store ADMIN_API_KEY in environment, never commit

**Web Scraping:**
- ‚úÖ JWT authentication required
- ‚úÖ Trusted domain filtering for medical content
- ‚úÖ Content validation and sanitization
- ‚úÖ Configurable depth limits (prevent abuse)
- ‚ö†Ô∏è Respect robots.txt and website terms of service

---

### Files Modified

**New Files:**
- `app/agent_keys.py` - Agent API key management (200 lines)
- `app/web_scraper.py` - Web scraping engine (350 lines)
- `app/web_scraping_routes.py` - Web scraping routes (250 lines)
- `AGENT_KEYS_WEB_SCRAPING_GUIDE.md` - Complete documentation (800+ lines)
- `setup_new_features.sh` - Automated setup script

**Modified Files:**
- `app/main.py` - Import agent keys and web scraping routes
- `requirements.txt` - Added beautifulsoup4, PyPDF2, aiohttp

**Dependencies:**
- beautifulsoup4==4.12.3 ‚úÖ Installed
- PyPDF2==3.0.1 ‚úÖ Installed
- aiohttp==3.11.11 ‚úÖ Installed

---

### Testing

**Test Agent Keys:**
```bash
# Check routes available
curl -s http://localhost:8000/openapi.json | grep -A 5 "agent-keys"

# Try creating key without admin key (should fail)
curl -X POST http://localhost:8000/v1/admin/agent-keys \
  -H "Content-Type: application/json" \
  -d '{"agent_type": "Claims"}'
# Expected: 401 Unauthorized
```

**Test Web Scraping:**
```bash
# Test scraping (requires auth)
TOKEN=$(curl -s -X POST http://localhost:8000/v1/auth/login \
  -H "Content-Type: application/json" \
  -d '{"username":"admin","password":"SecureAdmin2026!"}' \
  | python3 -c "import sys,json; print(json.load(sys.stdin)['access_token'])")

curl -X POST http://localhost:8000/v1/knowledge/scrape \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://example.com",
    "specialty": "general",
    "follow_links": false,
    "ingest_to_kb": false
  }'
```

---

### Next Steps

1. **Configure Production Keys**
   ```bash
   # Generate strong admin key
   export ADMIN_API_KEY=$(openssl rand -hex 32)
   echo "ADMIN_API_KEY=$ADMIN_API_KEY" >> .env
   pm2 restart api-gateway --update-env
   ```

2. **Create Agent Keys**
   ```bash
   ./setup_new_features.sh
   # Saves keys to .agent_keys file
   ```

3. **Populate Knowledge Base**
   ```bash
   # Scrape trusted medical sources
   # Examples in AGENT_KEYS_WEB_SCRAPING_GUIDE.md
   ```

4. **Update Web UI** (Optional)
   - Add agent key management interface
   - Add web scraping interface
   - See static/agent.html for integration

5. **Monitor Usage**
   ```bash
   pm2 logs api-gateway | grep -E "(agent-key|scrape)"
   ```

---

### Documentation

**Complete Guide:** `AGENT_KEYS_WEB_SCRAPING_GUIDE.md`
- Agent API key management
- Web scraping usage
- Training pipeline
- API reference
- Examples and troubleshooting

**Access:**
```bash
cat /home/dgs/N3090/services/inference-node/AGENT_KEYS_WEB_SCRAPING_GUIDE.md
```

---

### Status

‚úÖ **Agent API Keys**: Fully implemented and operational  
‚úÖ **Web Scraping**: Fully implemented and operational  
‚úÖ **Dependencies**: Installed (beautifulsoup4, PyPDF2, aiohttp)  
‚úÖ **API Routes**: Integrated and available  
‚úÖ **Documentation**: Complete guide created  
‚úÖ **Setup Script**: Automated setup available  
‚è≥ **Testing**: Ready for production testing  

**Current System Status:**
- API Gateway: ‚Ü∫8, online, 27.0mb
- All 6 PM2 processes: online and stable
- New routes available at `/v1/admin/agent-keys` and `/v1/knowledge/scrape*`
- Documentation: AGENT_KEYS_WEB_SCRAPING_GUIDE.md

**Web Interfaces:**
- Simple Chat: http://192.168.1.55:8000/static/index.html
- Agentic Platform: http://192.168.1.55:8000/static/agent.html
- (Agent key management and web scraping can be added to UI later)
